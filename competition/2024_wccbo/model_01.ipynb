{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5b09d4-0c06-4888-b744-e4e54538074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from geopy.distance import geodesic\n",
    "#import pandas_profiling as pdp\n",
    "import optuna\n",
    "import mlflow\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716544cd-431d-4a78-8578-df42292f65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: 268ced833f134275b33f04ddd7b338e9\n"
     ]
    }
   ],
   "source": [
    "# experiment_name = 'Signate_Sony_pm2.5' # 記録用\n",
    "experiment_name = 'tmp' # 試行錯誤用\n",
    "\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "run = mlflow.start_run()\n",
    "run_id = run.info.run_id\n",
    "print(\"Active run_id: {}\".format(run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b980c4-5597-4cf7-bfa8-cd454dab9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const:\n",
    "    '''\n",
    "    定数用のクラス。定数はココでのみ定義することとする。\n",
    "    mlflowで記録するのは、使うときだけ。\n",
    "    '''\n",
    "    # lgbm関連\n",
    "    do_use_saved_lgbmmodel = False\n",
    "    do_tuning_hypara_lgbm = False\n",
    "    do_training_lgbm = True\n",
    "    do_predict_lgbm = True\n",
    "    do_use_bestparam_lgbm = False\n",
    "    cv_n_splits_lgbm = 10\n",
    "    hypara_cv_n_splits_lgbm = 5\n",
    "    do_save_lgbmmodel = True\n",
    "    # tabnet関連\n",
    "    do_use_saved_tabnetmodel = False\n",
    "    do_tuning_hypara_tabnet = False\n",
    "    do_pretraining = False\n",
    "    do_training_tabnet = False\n",
    "    do_predict_tabnet = False\n",
    "    do_use_bestparam_tabnet = False\n",
    "    cv_n_splits_tabnet = 10\n",
    "    hypara_cv_n_splits_tabnet = 5\n",
    "    do_save_tabnetmodel = False\n",
    "    # アンサンブル関連\n",
    "    weight_lgbm = 0.5\n",
    "    weight_tabnet = 0.5\n",
    "    # その他\n",
    "    do_submit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd8dac2-2eec-493b-9aeb-e9679ed3eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期化\n",
    "y_pred_lgbm = None\n",
    "y_pred_tabnet = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ef2da6-3698-44d0-abc7-9d723cb69746",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath =  './01_data/train.csv'\n",
    "testpath =  './01_data/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(trainpath)\n",
    "test_df = pd.read_csv(testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a5307-ce87-45d4-a957-916fa0c41e62",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96e53f9-6af2-4a57-9f4d-54ae2b614d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df):\n",
    "    types = df.dtypes\n",
    "    for col in df.columns:\n",
    "        if types[col] == 'category':\n",
    "            print(col)\n",
    "            l_enc = LabelEncoder()\n",
    "            df[col] = l_enc.fit_transform(df[col].values)\n",
    "    return df\n",
    "\n",
    "def onehot_encoding(df):\n",
    "    df_ohe = pd.DataFrame([])\n",
    "    cond = df.dtypes[df.dtypes == 'category'].index\n",
    "    df_ohe = df[cond]\n",
    "    df_ohe = pd.get_dummies(df_ohe)\n",
    "    df = df.drop(cond, axis=1)\n",
    "    df = pd.concat([df, df_ohe], axis=1)\n",
    "    return df\n",
    "\n",
    "def commom_preprocessing(df):\n",
    "    # dtypeの変換\n",
    "    df['Country'] = df['Country'].astype('category')\n",
    "    df['City'] = df['City'].astype('category')\n",
    "    df['year'] = df['year'].astype('category')\n",
    "    return df\n",
    "\n",
    "def lgbm_preprocessing(df):\n",
    "    return df\n",
    "\n",
    "def tabnet_preprocessing(df):\n",
    "    df = label_encoding(df)\n",
    "    # df = onehot_encoding(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691791a0-3fb3-45bc-80ee-62ba7d425396",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = commom_preprocessing(train_df)\n",
    "test_df = commom_preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efeb2fde-94aa-4f07-9644-e2bbfeb9ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster(train_df, test_df, n_clusters, linkage):\n",
    "    '''\n",
    "    Args:\n",
    "        train_dfもtest_dfもある程度前処理を終わったものを渡す。\n",
    "    Returns:\n",
    "        train_df及びtest_df\n",
    "    '''\n",
    "    train_df['train'] = True\n",
    "    test_df['train'] = False\n",
    "    all_df = pd.concat([train_df, test_df])\n",
    "    \n",
    "    cities_df = pd.concat([train_df, test_df])[['Country', 'City', 'lat', 'lon']].drop_duplicates()\n",
    "    cities_df['spot'] = cities_df[['lat', 'lon']].apply(lambda x: (x[0], x[1]), axis=1)\n",
    "    world_cities = cities_df.sort_values('lat', ascending=False).reset_index(drop=True)    \n",
    "    n_world = len(world_cities)\n",
    "    print('n_world =', n_world)\n",
    "    # 都市間の距離行列を作る\n",
    "    world_mtx = np.zeros(n_world**2).reshape(n_world, n_world)\n",
    "    for i in range(n_world):\n",
    "        for j in range(n_world):\n",
    "            world_mtx[i,j] = geodesic(world_cities.at[i, 'spot'], world_cities.at[j, 'spot']).km # 距離の算出\n",
    "\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage).fit(world_mtx)\n",
    "    # 都市名、緯度、経度、クラスター番号を格納したDataFrameを作成\n",
    "    cluster_df = world_cities[['City', 'lat', 'lon']].copy()\n",
    "    cluster_df['cluster'] = clustering.labels_\n",
    "    # display(cluster_df)\n",
    "    g = cluster_df[['City', 'cluster']].set_index('City')['cluster']\n",
    "    all_df['cluster'] = all_df['City'].map(g)\n",
    "    # display(all_df)    \n",
    "    train_df = all_df[all_df['train']==True]\n",
    "    test_df = all_df[all_df['train']==False]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def add_country_month_aggregation(train_df, test_df):\n",
    "    \n",
    "    def rename_multicol(df):\n",
    "        df_col=df.columns #列名をコピー\n",
    "        df = df.T.reset_index(drop=False).T #一回列名をリセット\n",
    "        for  i in range(df.shape[1]): #列名を新たに定義\n",
    "            rename_col = {i:\"_Country_month_\".join(df_col[i])}\n",
    "            df = df.rename(columns = rename_col)     \n",
    "        df = df.drop([\"level_0\",\"level_1\"],axis=0)\n",
    "        return df\n",
    "    \n",
    "    train_df['train'] = True\n",
    "    test_df['train'] = False\n",
    "    all_df = pd.concat([train_df, test_df])\n",
    "    agg_columns = [c for c in train_df.columns if re.search(r'_max|_mid|_min|_var', c) and not re.search(r'pm25', c)]\n",
    "    agg_columns_plus = ['Country', 'month'] + agg_columns\n",
    "    agg_df = all_df[agg_columns_plus]\n",
    "    group_df = agg_df.groupby(['Country', 'month']).agg([np.mean, np.max, np.min, np.std])\n",
    "    group_df = rename_multicol(group_df)\n",
    "    \n",
    "    # print(len(group_df.columns))\n",
    "    # for c in group_df.columns:\n",
    "    #     print(c)\n",
    "    \n",
    "    for col in group_df.columns:\n",
    "        group_df[col] = group_df[col].astype('float64')\n",
    "    group_df_columns = group_df.columns\n",
    "    out_df = pd.merge(all_df, group_df, how='left', on=['Country', 'month'])\n",
    "    \n",
    "    train_df = out_df[out_df['train']==True]\n",
    "    test_df = out_df[out_df['train']==False]\n",
    "    return train_df, test_df, group_df_columns\n",
    "    \n",
    "def add_feature_common(df):\n",
    "    # 北半球 or 南半球\n",
    "    # df['hemisphere'] = (df['lat'] >= 0)\n",
    "    # # 通算日の計算\n",
    "    df['date'] = df['year'].astype(str)+'-'+df['month'].astype(str)+'-'+df['day'].astype(str)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # df['date_boy'] = pd.to_datetime(df['year'].astype(str)+'-01-01') # beginning of the year\n",
    "    # df['totaldate'] = df['date'] - df['date_boy']\n",
    "    df['totaldate'] = df['date'].dt.strftime(\"%j\").astype(int)\n",
    "    \n",
    "    # # 国別の平均CO中央値\n",
    "    # mean_co_mid = df.groupby('Country')['co_mid'].mean()\n",
    "    # df['country_average_co_mid'] = df['Country'].map(mean_co_mid).astype('float64')\n",
    "    # # 国別の最大CO中央値\n",
    "    # max_co_mid = df.groupby('Country')['co_mid'].max()\n",
    "    # df['country_max_co_mid'] = df['Country'].map(max_co_mid).astype('float64')\n",
    "    # # 国別の最小CO中央値\n",
    "    # min_co_mid = df.groupby('Country')['co_mid'].min()\n",
    "    # df['country_min_co_mid'] = df['Country'].map(min_co_mid).astype('float64')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_GDP(df):\n",
    "    return \n",
    "\n",
    "def add_feature_train(df):\n",
    "    # 国別の平均PM2.5中央値\n",
    "    # 参考：https://teratail.com/questions/204630\n",
    "    mean_pm25 = df.groupby('Country')['pm25_mid'].mean()\n",
    "    df['country_average_pm25_mid'] = df['Country'].map(mean_pm25).astype('float64')\n",
    "    # 国別の最大PM2.5中央値\n",
    "    max_pm25 = df.groupby('Country')['pm25_mid'].max()\n",
    "    df['country_max_pm25_mid'] = df['Country'].map(max_pm25).astype('float64')\n",
    "    # 国別の最小PM2.5中央値\n",
    "    min_pm25 = df.groupby('Country')['pm25_mid'].min()\n",
    "    df['country_min_pm25_mid'] = df['Country'].map(min_pm25).astype('float64')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def add_feature_test(df):\n",
    "    # 国別の平均PM2.5中央値\n",
    "    mean_pm25 = train_df.groupby('Country')['pm25_mid'].mean()\n",
    "    df['country_average_pm25_mid'] = df['Country'].map(mean_pm25).astype('float64')\n",
    "    # 国別の最大PM2.5中央値\n",
    "    max_pm25 = train_df.groupby('Country')['pm25_mid'].max()\n",
    "    df['country_max_pm25_mid'] = df['Country'].map(max_pm25).astype('float64')\n",
    "    # 国別の最小PM2.5中央値\n",
    "    min_pm25 = train_df.groupby('Country')['pm25_mid'].min()\n",
    "    df['country_min_pm25_mid'] = df['Country'].map(min_pm25).astype('float64')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc98791-88f7-41f3-ba51-49993a345d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = add_cluster(\n",
    "#     train_df,\n",
    "#     test_df,\n",
    "#     n_clusters = 150,\n",
    "#     linkage = 'single'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2bf1472-6d64-41cc-a17c-419a4aeb42aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6721/1531721425.py:44: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  df = df.drop([\"level_0\",\"level_1\"],axis=0)\n"
     ]
    }
   ],
   "source": [
    "train_df = add_feature_common(train_df)\n",
    "test_df = add_feature_common(test_df)\n",
    "train_df, test_df, group_df_columns = add_country_month_aggregation(train_df, test_df)\n",
    "\n",
    "# train_df = add_feature_train(train_df)\n",
    "# test_df = add_feature_test(test_df)\n",
    "\n",
    "train_df['Country'] = train_df['Country'].astype('category')\n",
    "test_df['Country'] = test_df['Country'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8668da54-b4b5-488d-b771-212931991770",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "#    'id',\n",
    "#    'year',\n",
    "#    'month',\n",
    "#    'day',\n",
    "    'Country',\n",
    "#    'City',\n",
    "#    'cluster',\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'co_cnt',\n",
    "    'co_min',\n",
    "    'co_mid',\n",
    "    'co_max',\n",
    "    'co_var',\n",
    "    'o3_cnt',\n",
    "    'o3_min',\n",
    "    'o3_mid',\n",
    "    'o3_max',\n",
    "    'o3_var',\n",
    "    'so2_cnt',\n",
    "    'so2_min',\n",
    "    'so2_mid',\n",
    "    'so2_max',\n",
    "    'so2_var',\n",
    "    'no2_cnt',\n",
    "    'no2_min',\n",
    "    'no2_mid',\n",
    "    'no2_max',\n",
    "    'no2_var',\n",
    "    'temperature_cnt',\n",
    "    'temperature_min',\n",
    "    'temperature_mid',\n",
    "    'temperature_max',\n",
    "    'temperature_var',\n",
    "    'humidity_cnt',\n",
    "    'humidity_min',\n",
    "    'humidity_mid',\n",
    "    'humidity_max',\n",
    "    'humidity_var',\n",
    "    'pressure_cnt',\n",
    "    'pressure_min',\n",
    "    'pressure_mid',\n",
    "    'pressure_max',\n",
    "    'pressure_var',\n",
    "    'ws_cnt',\n",
    "    'ws_min',\n",
    "    'ws_mid',\n",
    "    'ws_max',\n",
    "    'ws_var',\n",
    "    'dew_cnt',\n",
    "    'dew_min',\n",
    "    'dew_mid',\n",
    "    'dew_max',\n",
    "    'dew_var',\n",
    "    #'pm25_mid',\n",
    "    \n",
    "    # additional feature\n",
    "    #'hemisphere', # 全く効いてない。\n",
    "    'totaldate',\n",
    "    #'country_average_pm25_mid', # あまり効いてない。\n",
    "    #'country_max_pm25_mid',\n",
    "    #'country_min_pm25_mid',\n",
    "    #'country_max_co_mid',\n",
    "    #'country_min_co_mid',\n",
    "    \n",
    "    *group_df_columns,\n",
    "\n",
    "]\n",
    "mlflow.log_param('features', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36aac867-ef89-4d8d-ba68-758714e15e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[features]\n",
    "Y = train_df['pm25_mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08933cdc-49cf-4626-9ab9-0be84697a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195941, 193)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ed457-83f4-4a98-9662-2656c6a7bf38",
   "metadata": {},
   "source": [
    "## To use or not use pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a534a0-6e02-4b73-97d3-772cefd8b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Const.do_use_saved_lgbmmodel:\n",
    "    modelname = 'model_id_bbd6408b112a45a383ef7543aae38945.pickle'\n",
    "    path = f'models/lgbm/{modelname}'\n",
    "    with open(path, 'rb') as f:\n",
    "        models_lgbm = pickle.load(f)\n",
    "\n",
    "if Const.do_use_saved_tabnetmodel:\n",
    "    # modelname = 'model_id_7f4d511735a7449fb9a4b16e35be8773.pickle'\n",
    "    modelname = 'model_id_d466589a2fb3443eb2bbb83be454db84.pickle'\n",
    "    path = f'models/tabnet/{modelname}'\n",
    "    with open(path, 'rb') as f:\n",
    "        models_tabnet = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368a1cd-8dd1-40b2-8e35-58d20e0a9094",
   "metadata": {},
   "source": [
    "---\n",
    "## training with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8b38e67-0656-4201-b123-30da6f8e5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter tuning\n",
    "def opt_lgbm(trial):\n",
    "    '''\n",
    "    hyper-parameter tuning with optuna.\n",
    "    \n",
    "    Ref:\n",
    "        https://datadriven-rnd.com/lightgbm/\n",
    "        https://kiroka-camp.com/lightgbm-optuna\n",
    "        https://meknowledge.jpn.org/2021/05/28/lightgbm-optuna-tuning/\n",
    "        https://knknkn.hatenablog.com/entry/2021/06/29/125226#learning_rate\n",
    "        https://knknkn.hatenablog.com/entry/2021/06/14/160302\n",
    "    '''\n",
    "    mlflow.log_param('Const.hypara_cv_n_splits_lgbm', Const.hypara_cv_n_splits_lgbm)\n",
    "    num_leaves = trial.suggest_int('num_leaves', 2, 1024)\n",
    "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.4, 1.0)\n",
    "    feature_fraction = trial.suggest_float('feature_fraction', 0.4, 1.0)\n",
    "    lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-8, 10.0)\n",
    "    lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.05,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': num_leaves,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'n_estimators': 10000,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "    }\n",
    "    score = 0\n",
    "    scores = []\n",
    "    # gkf = GroupKFold(n_splits=Const.hypara_cv_n_splits_lgbm)\n",
    "    gkf = KFold(n_splits=Const.hypara_cv_n_splits_lgbm)\n",
    "    #for train_idx, valid_idx in gkf.split(X, Y, groups=train_df['City']):\n",
    "    for train_idx, valid_idx in gkf.split(X):\n",
    "        hypara_estimator = lgb.LGBMRegressor(**param)\n",
    "        x_train, x_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "        hypara_estimator.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set = [(x_train, y_train), (x_valid, y_valid)],\n",
    "            early_stopping_rounds = 10,\n",
    "            feature_name = features,\n",
    "            verbose=1000\n",
    "        )\n",
    "        y_pred = hypara_estimator.predict(x_valid)\n",
    "        score = np.sqrt(metrics.mean_squared_error(y_valid, y_pred))\n",
    "        scores.append(score)\n",
    "    score = np.array(scores).mean()\n",
    "    return score        \n",
    "\n",
    "def tuning_hypara_lgbm():\n",
    "    study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0), direction='minimize')\n",
    "    study.optimize(opt_lgbm, n_trials=100)\n",
    "    bestparam = study.best_params\n",
    "    return bestparam\n",
    "    \n",
    "# この部分は後ろの方に持ってきてもいいかも。\n",
    "mlflow.log_param('Const.do_tuning_hypara_lgbm', Const.do_tuning_hypara_lgbm)\n",
    "if Const.do_tuning_hypara_lgbm:\n",
    "    bestparam = tuning_hypara_lgbm()\n",
    "    print('bestparam =', bestparam)\n",
    "    mlflow.log_param('bestparam', bestparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dccd88bb-9965-4c3b-b03f-704bebbf4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param('Const.do_use_bestparam_lgbm', Const.do_use_bestparam_lgbm)\n",
    "if Const.do_use_bestparam_lgbm:\n",
    "    #bestparam = {'num_leaves': 503, 'bagging_fraction': 0.9936458663706589, 'feature_fraction': 0.43918252429106813, 'lambda_l1': 0.11197213132290361, 'lambda_l2': 3.940818887528048e-06}\n",
    "    #bestparam = {'num_leaves': 379, 'bagging_fraction': 0.8925959379087611, 'feature_fraction': 0.4582607654758368, 'lambda_l1': 0.34793984646334997, 'lambda_l2': 7.326316408987164e-08}\n",
    "    \n",
    "    # KFoldに変更。Contry,monthのagg特徴量追加前。\n",
    "    #bestparam = {'num_leaves': 327, 'bagging_fraction': 0.800446227978209, 'feature_fraction': 0.4790787174426353, 'lambda_l1': 0.027986258230304234, 'lambda_l2': 4.0239709475948286e-06}　# \n",
    "\n",
    "    # Contry,monthのagg特徴量追してハイパラサーチ。100回trialしたが、35回でkernelダウン。その中で最適値を選択。\n",
    "    bestparam = {'num_leaves': 379, 'bagging_fraction': 0.8925959379087611, 'feature_fraction': 0.4582607654758368, 'lambda_l1': 0.34793984646334997, 'lambda_l2': 7.326316408987164e-08}\n",
    "    pass\n",
    "else:\n",
    "    bestparam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "329684ff-a1af-4caf-a077-d5e2de42d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(bestparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aabd978-e150-40f3-8f2e-99fec81d1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_with_lgbm(X, Y, bestparam=None):\n",
    "    '''\n",
    "    LightGBMで学習。\n",
    "    \n",
    "    Args:\n",
    "        X,\n",
    "        Y,\n",
    "        bestparams,\n",
    "    Returns:\n",
    "        models,\n",
    "        \n",
    "    '''\n",
    "    mlflow.log_param('Const.cv_n_splits_lgbm', Const.cv_n_splits_lgbm)\n",
    "\n",
    "    models = []\n",
    "    cvscores = []\n",
    "    y_valid_dfs = []\n",
    "    y_valid_predict_dfs = []\n",
    "    vis_valid_df = pd.DataFrame(columns=['id', 'year', 'month', 'day', 'Country' ,'City', 'pm25_mid', 'pm25_mid_predict'])\n",
    "\n",
    "    #gkf = GroupKFold(n_splits=Const.cv_n_splits_lgbm)\n",
    "    gkf = KFold(n_splits=Const.cv_n_splits_lgbm, random_state=42, shuffle=True)\n",
    "    #for n_fold, (train_idx, valid_idx) in enumerate(gkf.split(X, Y, groups=train_df['City'])):\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(gkf.split(X)):\n",
    "        print('n_fold =', n_fold)\n",
    "\n",
    "        if Const.do_use_bestparam_lgbm:\n",
    "            model = lgb.LGBMRegressor(**bestparam)\n",
    "            mlflow.log_param('bestparam', bestparam)\n",
    "        else:\n",
    "            param = {\n",
    "                'objective': 'regression',\n",
    "                'learning_rate': 0.05,\n",
    "                'boosting_type': 'gbdt',\n",
    "                'metric': 'rmse',\n",
    "                'n_estimators': 10000,\n",
    "                'lambda_l1': 1.0,\n",
    "                'lambda_l2': 0.0,\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**param)\n",
    "            mlflow.log_param('param', param)\n",
    "        x_train, x_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "\n",
    "        model.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set = [(x_train, y_train), (x_valid, y_valid)],\n",
    "            verbose=1000,\n",
    "            early_stopping_rounds=10,    \n",
    "        )\n",
    "\n",
    "        valid_df = pd.DataFrame(columns=['id', 'year', 'month', 'day', 'Country' ,'City', 'pm25_mid', 'pm25_mid_predict'])\n",
    "        valid_df['id'] = train_df.loc[valid_idx, 'id']\n",
    "        valid_df['year'] = train_df.loc[valid_idx, 'year']\n",
    "        valid_df['month'] = train_df.loc[valid_idx, 'month']\n",
    "        valid_df['day'] = train_df.loc[valid_idx, 'day']\n",
    "        valid_df['Country'] = train_df.loc[valid_idx, 'Country']\n",
    "        valid_df['City'] = train_df.loc[valid_idx, 'City']\n",
    "\n",
    "        y_valid_predict = model.predict(x_valid)\n",
    "        cvscore = np.sqrt(metrics.mean_squared_error(y_valid, y_valid_predict))\n",
    "        valid_df['pm25_mid'] = y_valid\n",
    "        valid_df['pm25_mid_predict'] = y_valid_predict\n",
    "        vis_valid_df = pd.concat([vis_valid_df, valid_df])\n",
    "        models.append(model)\n",
    "        cvscores.append(cvscore)\n",
    "        \n",
    "    return models, cvscores, vis_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65734fbd-1859-4865-9e98-24c6b660e5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Australia\n",
       "1             Australia\n",
       "2             Australia\n",
       "3             Australia\n",
       "4             Australia\n",
       "              ...      \n",
       "195936    United States\n",
       "195937    United States\n",
       "195938    United States\n",
       "195939          Vietnam\n",
       "195940          Vietnam\n",
       "Name: Country, Length: 195941, dtype: category\n",
       "Categories (30, object): ['Australia', 'Belgium', 'Bosnia and Herzegovina', 'Brazil', ..., 'Turkey', 'United Kingdom', 'United States', 'Vietnam']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b222b89-78c3-409d-bded-b6c5a3771309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fold = 0\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\ttraining's rmse: 17.0204\tvalid_1's rmse: 19.4133\n",
      "Early stopping, best iteration is:\n",
      "[1128]\ttraining's rmse: 16.7401\tvalid_1's rmse: 19.3706\n",
      "n_fold = 1\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\ttraining's rmse: 17.0136\tvalid_1's rmse: 19.4991\n",
      "Early stopping, best iteration is:\n",
      "[1094]\ttraining's rmse: 16.8072\tvalid_1's rmse: 19.4601\n",
      "n_fold = 2\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[988]\ttraining's rmse: 17.0429\tvalid_1's rmse: 19.4097\n",
      "n_fold = 3\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\ttraining's rmse: 17.0384\tvalid_1's rmse: 19.3859\n",
      "Early stopping, best iteration is:\n",
      "[1300]\ttraining's rmse: 16.3841\tvalid_1's rmse: 19.2753\n",
      "n_fold = 4\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\ttraining's rmse: 17.0205\tvalid_1's rmse: 19.2818\n",
      "Early stopping, best iteration is:\n",
      "[1593]\ttraining's rmse: 15.8253\tvalid_1's rmse: 19.1087\n",
      "n_fold = 5\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\ttraining's rmse: 17.0038\tvalid_1's rmse: 19.6445\n",
      "Early stopping, best iteration is:\n",
      "[1290]\ttraining's rmse: 16.3832\tvalid_1's rmse: 19.5635\n",
      "n_fold = 6\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\ttraining's rmse: 17.025\tvalid_1's rmse: 19.4003\n",
      "Early stopping, best iteration is:\n",
      "[1106]\ttraining's rmse: 16.7828\tvalid_1's rmse: 19.3574\n",
      "n_fold = 7\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\ttraining's rmse: 16.9943\tvalid_1's rmse: 19.6692\n",
      "Early stopping, best iteration is:\n",
      "[1414]\ttraining's rmse: 16.1469\tvalid_1's rmse: 19.528\n",
      "n_fold = 8\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[646]\ttraining's rmse: 17.9574\tvalid_1's rmse: 19.5464\n",
      "n_fold = 9\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/user/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[903]\ttraining's rmse: 17.2703\tvalid_1's rmse: 19.4685\n",
      "cvscores_lgbm = [19.370598761366786, 19.460137896887712, 19.409679971584257, 19.27526875106481, 19.108663827559635, 19.563459633336613, 19.35737174025195, 19.527986871310404, 19.546391352612783, 19.468511503757963]\n",
      "average = 19.408807030973286\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_param('Const.do_training_lgbm', Const.do_training_lgbm)\n",
    "if Const.do_training_lgbm:\n",
    "    models_lgbm, cvscores, vis_valid_df = training_with_lgbm(X, Y, bestparam)\n",
    "    average = np.array(cvscores).mean()\n",
    "    print('cvscores_lgbm =', cvscores)\n",
    "    print('average =', average)\n",
    "    for i in range(len(cvscores)):\n",
    "        mlflow.log_metric(f'cvscores_lgbm{i}', cvscores[i])\n",
    "    mlflow.log_metric('average', average)\n",
    "    \n",
    "    #average = 22.12377600122617 fold数= 20 learning_rate = 0.1\n",
    "    #average = 22.023999283119117 fold数= 10 learning_rate = 0.05 ←　ベースラインとする。\n",
    "    #average = 22.183219604203924 fold数= 20 learning_rate = 0.05\n",
    "    # average = 21.895404967428846 fold= 10 learning_rate = 0.05 ハイパラtuningあり。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f4de7d-3e3b-415e-a90f-fbf866e94999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelのpickle保存\n",
    "if Const.do_save_lgbmmodel:\n",
    "    savepath = f'models/lgbm/model_id_{run_id}.pickle'\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump(models_lgbm , f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4bd87-81ee-47cc-a2b3-c408a855f985",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10b53703-ae7c-4831-b173-3d02e75321eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualization:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def vis_result_varidation(self, df, Country, City):\n",
    "        '''\n",
    "        validationの結果の可視化。指定した国・都市のpm25_midの値について、\n",
    "        validationの結果と真値を比較する。\n",
    "        \n",
    "        Args:\n",
    "            df(pandas.DataFrame): 可視化対象のデータフレーム。\n",
    "            Country(string): 国名\n",
    "            City(string): 都市名\n",
    "        '''\n",
    "        cond = (df['Country'] == Country) & (df['City'] == City)\n",
    "        df = df[cond]\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(30, 8))\n",
    "        ax.set_title(f'{Country}, {City}')\n",
    "        ax.plot(df['pm25_mid'])\n",
    "        ax.plot(df['pm25_mid_predict'])\n",
    "        plt.show()\n",
    "        \n",
    "    def vis_feature_importance(self, models):\n",
    "        '''\n",
    "        feature_importanceの可視化\n",
    "        '''\n",
    "        for model in models:\n",
    "            lgb.plot_importance(model, figsize=(30, 40))\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeb95b57-ba15-421f-b097-414719d940e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Visualization()\n",
    "# v.vis_result_varidation(vis_valid_df, 'Australia', 'Brisbane')\n",
    "# v.vis_result_varidation(vis_valid_df, 'China', 'Beijing')\n",
    "# v.vis_result_varidation(vis_valid_df, 'Japan', 'Yokohama')\n",
    "# v.vis_feature_importance(models_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31857dc-b529-48d8-bfdf-b634c90b167b",
   "metadata": {},
   "source": [
    "## pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91969667-37c7-422d-8f45-2ceb469525b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_labeling(models, n_repeat=1, ):\n",
    "    '''\n",
    "    pseudo labelingを実施し、新しくmodelを作る。\n",
    "    \n",
    "    Args:\n",
    "        models(list): pseudo labelingする前のモデル。\n",
    "        n_repeat(int): pseudo labelingを実行する回数。\n",
    "    Returns:\n",
    "        models(list): pseudo labelingしたデータフレームで学習したモデル。\n",
    "\n",
    "    '''\n",
    "    # 普通に推論してy_pred出す。\n",
    "    y_pred = 0\n",
    "    for i in range(len(models)):\n",
    "        x_test = test_df[features]\n",
    "        y_pred += models[i].predict(x_test)\n",
    "    y_pred /= len(models)    \n",
    "    \n",
    "    # 学習データに　x_test, y_predを付け加える。\n",
    "    \n",
    "    print('[y_pred]')\n",
    "    display(y_pred)\n",
    "    \n",
    "    print('train_df[features]')\n",
    "    display(train_df[features])\n",
    "    print()\n",
    "    print('test_df[features]')\n",
    "    display(test_df[features])\n",
    "    print()\n",
    "    print('X')\n",
    "    \n",
    "    X = pd.DataFrame()\n",
    "    X = pd.concat([train_df[features], test_df[features]]) # reset indexが必要。  \n",
    "    \n",
    "    display(X)\n",
    "    \n",
    "\n",
    "    assert False\n",
    "\n",
    "    Y = pd.DataFrame()\n",
    "    Y = pd.concat(train_df['pm25_mid'], )\n",
    "    \n",
    "    Y = train_df['pm25_mid']\n",
    "    \n",
    "    # 新しく作ったtrainに対して、cvしてmodelを作る。\n",
    "    # (ココでもハイパラサーチ必要か？)\n",
    "    \n",
    "    \n",
    "    # コレをn_repeat回繰り返す。\n",
    "    # modelsを返す。\n",
    "    \n",
    "\n",
    "# pseudo_labeling(models)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cb265-c987-440c-a11c-e74ec294df9d",
   "metadata": {},
   "source": [
    "## predict with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a6a42bd-9ecb-4f39-89b9-f6b16dfdb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lgbm(models, x_test):\n",
    "    '''\n",
    "    LightGBMで学習したモデルで推論する。\n",
    "    '''\n",
    "    y_pred = 0\n",
    "    for i in range(len(models)):\n",
    "        x_test = test_df[features]\n",
    "        y_pred += models[i].predict(x_test)\n",
    "    y_pred /= len(models)    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a35664b9-2b34-45e1-b895-36954d09a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param('Const.do_predict_lgbm', Const.do_predict_lgbm)\n",
    "if Const.do_predict_lgbm:\n",
    "    x_test = test_df[features]\n",
    "    y_pred_lgbm = predict_lgbm(models_lgbm, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55915b18-98f1-4416-af85-6116b91d15e2",
   "metadata": {},
   "source": [
    "---\n",
    "## training with TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d154e16-a20f-46b7-921d-758bbe4d3f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6721/954852036.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = l_enc.fit_transform(df[col].values)\n"
     ]
    }
   ],
   "source": [
    "# train_df = tabnet_preprocessing(train_df)\n",
    "# test_df = tabnet_preprocessing(test_df)\n",
    "# X = train_df[features]\n",
    "# Y = train_df['pm25_mid']\n",
    "\n",
    "# train\n",
    "X = train_df[features]\n",
    "X = tabnet_preprocessing(X)\n",
    "Y = train_df['pm25_mid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a90e6f4-6ffb-4fd3-b847-4bb65e4aa708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_tabnet(trial):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "566e740b-500d-460e-ba3f-58a8c731576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param('Const.do_use_bestparam_tabnet', Const.do_use_bestparam_tabnet)\n",
    "if Const.do_use_bestparam_lgbm:\n",
    "    bestparam = None\n",
    "else:\n",
    "    bestparam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cdc8361-78ce-4fd5-b51f-b002d11cbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraining(X, Y):\n",
    "    '''\n",
    "    事前学習\n",
    "    '''\n",
    "    random_state = 0\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.3, random_state=random_state)\n",
    "    x_train = x_train.values.astype(np.float64)\n",
    "    x_valid = x_valid.values.astype(np.float64)\n",
    "    \n",
    "    unsupervised_model = TabNetPretrainer(\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        mask_type='entmax' # \"sparsemax\"\n",
    "    )\n",
    "    unsupervised_model.fit(\n",
    "        X_train=x_train,\n",
    "        eval_set=[x_valid], \n",
    "        pretraining_ratio=0.8,\n",
    "    )\n",
    "    return unsupervised_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed276f4a-4fd6-4759-bd6d-33616a7835a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param('Const.do_pretraining', Const.do_pretraining)\n",
    "if Const.do_pretraining:\n",
    "    unsupervised_model = pretraining(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8e1508f-edff-4211-ba59-182e07b0bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_with_tabnet(X, Y, bestparam=None, from_unsupervised=None):\n",
    "    models = []\n",
    "    cvscores = []\n",
    "    y_valid_dfs = []\n",
    "    y_valid_predict_dfs = []\n",
    "    vis_valid_df = pd.DataFrame(columns=['id', 'year', 'month', 'day', 'Country' ,'City', 'pm25_mid', 'pm25_mid_predict'])\n",
    "    \n",
    "    mlflow.log_param('Const.cv_n_splits_tabnet', Const.cv_n_splits_tabnet)\n",
    "    # gkf = GroupKFold(n_splits=Const.cv_n_splits_tabnet)\n",
    "    gkf = KFold(n_splits=Const.cv_n_splits_tabnet, random_state=40, shuffle=True)\n",
    "    #for n_fold, (train_idx, valid_idx) in enumerate(gkf.split(X, Y, groups=train_df['City'])):\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(gkf.split(X)):\n",
    "        print('n_fold =', n_fold)\n",
    "\n",
    "        if Const.do_use_bestparam_tabnet:\n",
    "            model = TabNetRegressor()\n",
    "        else:\n",
    "            tabnet_params = {\n",
    "                'optimizer_fn': torch.optim.Adam, \n",
    "                'optimizer_params': {'lr': 2e-2},\n",
    "                'n_steps': 3,\n",
    "                'scheduler_params': {'step_size': 10, 'gamma': 0.9}, \n",
    "                'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n",
    "            }\n",
    "            mlflow.log_param('tabnet_params', tabnet_params)\n",
    "            model = TabNetRegressor(**tabnet_params)\n",
    "        x_train, x_valid = X.iloc[train_idx].values, X.iloc[valid_idx].values\n",
    "        y_train, y_valid = Y.iloc[train_idx].values, Y.iloc[valid_idx].values\n",
    "\n",
    "        x_train = x_train.astype(np.float64)\n",
    "        x_valid = x_valid.astype(np.float64)\n",
    "        y_train = y_train.reshape(-1, 1).astype(np.float64)\n",
    "        y_valid = y_valid.reshape(-1, 1).astype(np.float64)\n",
    "\n",
    "        model.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set = [(x_train, y_train), (x_valid, y_valid)],\n",
    "            eval_metric=['rmse'],\n",
    "            from_unsupervised=unsupervised_model\n",
    "        )\n",
    "\n",
    "        valid_df = pd.DataFrame(columns=['id', 'year', 'month', 'day', 'Country' ,'City', 'pm25_mid', 'pm25_mid_predict'])\n",
    "        valid_df['id'] = train_df.loc[valid_idx, 'id']\n",
    "        valid_df['year'] = train_df.loc[valid_idx, 'year']\n",
    "        valid_df['month'] = train_df.loc[valid_idx, 'month']\n",
    "        valid_df['day'] = train_df.loc[valid_idx, 'day']\n",
    "        valid_df['Country'] = train_df.loc[valid_idx, 'Country']\n",
    "        valid_df['City'] = train_df.loc[valid_idx, 'City']\n",
    "\n",
    "        y_valid_predict = model.predict(x_valid)\n",
    "        cvscore = np.sqrt(metrics.mean_squared_error(y_valid, y_valid_predict))\n",
    "        valid_df['pm25_mid'] = y_valid\n",
    "        valid_df['pm25_mid_predict'] = y_valid_predict    \n",
    "        vis_valid_df = pd.concat([vis_valid_df, valid_df])\n",
    "        models.append(model)\n",
    "        cvscores.append(cvscore)\n",
    "    \n",
    "    return models, cvscores, vis_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2703138-e617-4bb7-8cb7-05246cf3c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param('Const.do_training_tabnet', Const.do_training_tabnet)\n",
    "if Const.do_training_tabnet:\n",
    "    if Const.do_pretraining:\n",
    "        models_tabnet, cvscores, vis_valid_df = training_with_tabnet(X, Y, bestparam, unsupervised_model)\n",
    "    else:\n",
    "        models_tabnet, cvscores, vis_valid_df = training_with_tabnet(X, Y, bestparam)\n",
    "    average = np.array(cvscores).mean()\n",
    "    print('cvscores_tabnet =', cvscores)\n",
    "    print('average =', average)\n",
    "    for i in range(len(cvscores)):\n",
    "        mlflow.log_metric(f'cvscores_tabnet{i}', cvscores[i])\n",
    "    mlflow.log_metric('average', average)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e27ec4b-2cb4-419a-959a-12d40da7894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabnetでのFeature importance\n",
    "def feature_importance_tabnet(models):\n",
    "    for model in models:\n",
    "        display(model.feature_importances_)\n",
    "\n",
    "# feature_importance_tabnet(models_tabnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c31bb14-e847-4545-80f7-60d99a03241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelのpickle保存\n",
    "if Const.do_save_tabnetmodel:\n",
    "    savepath = f'models/tabnet/model_id_{run_id}.pickle'\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump(models_tabnet , f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bfef4-da2d-4b08-8ca2-412f5ec4ca85",
   "metadata": {},
   "source": [
    "## predict with TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fe5a28c-e134-4b84-b3b5-7d64d341699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tabnet(models, x_test):\n",
    "    '''\n",
    "    LightGBMで学習したモデルで推論する。\n",
    "    \n",
    "    Args:\n",
    "        models(list): fold数分のmodel\n",
    "        x_test(numpy.array): 推論対象のデータ。numpy形式かつ、dtype=np.float64とする。 \n",
    "    '''\n",
    "    y_pred = 0\n",
    "    for i in range(len(models)):\n",
    "        y_pred += models[i].predict(x_test)\n",
    "    y_pred /= len(models)    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81476a70-8451-426e-8bfe-1b4f014fdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param('Const.do_predict_tabnet', Const.do_predict_tabnet)\n",
    "if Const.do_predict_tabnet:\n",
    "    x_test = tabnet_preprocessing(test_df[features]).values.astype(np.float64)\n",
    "    y_pred_tabnet = predict_tabnet(models_tabnet, x_test)\n",
    "    display(y_pred_tabnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f22214cd-156c-4017-be25-2a7fa826b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_tabnet\n",
    "# array([[21.556034],\n",
    "#        [45.753437],\n",
    "#        [33.175117],\n",
    "#        ...,\n",
    "#        [86.51942 ],\n",
    "#        [36.285927],\n",
    "#        [41.747646]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac86d74-f151-40d9-bf57-1791be71246c",
   "metadata": {},
   "source": [
    "---\n",
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7af54061-0c67-4e83-a316-01d6222f7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(y_pred_lgbm=None, y_pred_tabnet=None, weight_lgbm=None, weight_tabnet=None):\n",
    "    '''\n",
    "    '''\n",
    "    # if Const.do_predict_lgbm and Const.do_predict_tabnet:\n",
    "    #     y_pred_tabnet = y_pred_tabnet.reshape(-1)\n",
    "    #     y_pred_ensemble = weight_lgbm*y_pred_lgbm + weight_tabnet*y_pred_tabnet\n",
    "    # if Const.do_predict_lgbm and not Const.do_predict_tabnet:\n",
    "    #     y_pred_ensemble = y_pred_lgbm\n",
    "    # if not Const.do_predict_lgbm and Const.do_predict_tabnet:\n",
    "    #     y_pred_tabnet = y_pred_tabnet.reshape(-1)\n",
    "    #     y_pred_ensemble = y_pred_tabnet\n",
    "        \n",
    "    if y_pred_lgbm is not None and y_pred_tabnet is not None:\n",
    "        y_pred_tabnet = y_pred_tabnet.reshape(-1)\n",
    "        y_pred_ensemble = weight_lgbm*y_pred_lgbm + weight_tabnet*y_pred_tabnet    \n",
    "    if y_pred_lgbm is not None and y_pred_tabnet is None:\n",
    "        y_pred_ensemble = y_pred_lgbm\n",
    "    if y_pred_lgbm is None and y_pred_tabnet is not None:\n",
    "        y_pred_tabnet = y_pred_tabnet.reshape(-1)\n",
    "        y_pred_ensemble = y_pred_tabnet        \n",
    "    return y_pred_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c370004-a9de-4b3a-8893-2d975090993d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.37494306, 36.42463268, 26.50811286, ..., 65.03478816,\n",
       "       35.72041752, 38.73329297])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Const.do_predict_lgbm or Const.do_predict_tabnet:\n",
    "    mlflow.log_param('Const.weight_lgbm', Const.weight_lgbm)\n",
    "    mlflow.log_param('Const.weight_tabnet', Const.weight_tabnet)\n",
    "    y_pred_ensemble = ensemble(\n",
    "        y_pred_lgbm = y_pred_lgbm, \n",
    "        y_pred_tabnet = y_pred_tabnet,\n",
    "        weight_lgbm = Const.weight_lgbm,\n",
    "        weight_tabnet = Const.weight_tabnet\n",
    "    )\n",
    "    display(y_pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2b1a198-d4f1-48b0-98a2-9e6eb2b9ff2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Const.do_predict_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b0123-b7d9-4fd0-bdad-d883a0026b51",
   "metadata": {},
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a77c378-dc92-47cc-a568-06509d9fceb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pm25_mid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195941</th>\n",
       "      <td>195942</td>\n",
       "      <td>18.374943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195942</th>\n",
       "      <td>195943</td>\n",
       "      <td>36.424633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195943</th>\n",
       "      <td>195944</td>\n",
       "      <td>26.508113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195944</th>\n",
       "      <td>195945</td>\n",
       "      <td>63.742432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195945</th>\n",
       "      <td>195946</td>\n",
       "      <td>129.244098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249445</th>\n",
       "      <td>249446</td>\n",
       "      <td>70.990942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249446</th>\n",
       "      <td>249447</td>\n",
       "      <td>97.478673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249447</th>\n",
       "      <td>249448</td>\n",
       "      <td>65.034788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249448</th>\n",
       "      <td>249449</td>\n",
       "      <td>35.720418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249449</th>\n",
       "      <td>249450</td>\n",
       "      <td>38.733293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53509 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    pm25_mid\n",
       "195941  195942   18.374943\n",
       "195942  195943   36.424633\n",
       "195943  195944   26.508113\n",
       "195944  195945   63.742432\n",
       "195945  195946  129.244098\n",
       "...        ...         ...\n",
       "249445  249446   70.990942\n",
       "249446  249447   97.478673\n",
       "249447  249448   65.034788\n",
       "249448  249449   35.720418\n",
       "249449  249450   38.733293\n",
       "\n",
       "[53509 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if Const.do_submit:\n",
    "    submission_df = pd.DataFrame(columns=['id', 'pm25_mid'])\n",
    "    submission_df['id'] = test_df['id'].copy()\n",
    "    submission_df['pm25_mid'] = y_pred_ensemble\n",
    "    display(submission_df)\n",
    "    \n",
    "    if Const.do_predict_lgbm and Const.do_predict_tabnet:\n",
    "        save_filename = f'02_forSubmission/prediction_ensemble_No_{run_id}.csv'\n",
    "    if Const.do_predict_lgbm and not Const.do_predict_tabnet:\n",
    "        save_filename = f'02_forSubmission/prediction_lgbm_No_{run_id}.csv'\n",
    "    if not Const.do_predict_lgbm and Const.do_predict_tabnet:\n",
    "        save_filename = f'02_forSubmission/prediction_tabnet_No_{run_id}.csv'\n",
    "    \n",
    "    submission_df.to_csv(save_filename, header=False, index=False)\n",
    "    mlflow.log_artifact(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "363ab29e-a836-420f-b219-18701e7174f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b53b6e-a2f9-4c1e-93f2-ec2a745809a7",
   "metadata": {},
   "source": [
    "## memo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddff2e1-fe5b-42e1-bbc5-4ee539e26166",
   "metadata": {},
   "source": [
    "- `Country`及び`City`をcategoricalにして特徴量に入れる場合。\n",
    "    - cvscores = [31.168381258519616, 23.62248117843837, 25.045370727445352, 24.926670022160398, 26.691971608354752]\n",
    "    - average = 26.290974958983696\n",
    "- `Country`を特徴量に入れ、`City`は特徴量に入れない場合。\n",
    "    - cvscores = [24.50825768668205, 21.316029448606567, 22.10093074317413, 21.912565563021566, 22.08565642883814]\n",
    "    - average = 22.384687974064487\n",
    "- `Country`及び`City`を共に特徴量に入れない場合。\n",
    "    - cvscores = [25.003064401106673, 22.049348169514104, 21.934097643562655, 22.29729700536064, 22.38822735801613]\n",
    "    - average = 22.73440691551204"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62382778-41f2-436b-9e14-f440d98bd2b7",
   "metadata": {},
   "source": [
    "- `City`とcnt系を全て削除した場合。\n",
    "    - cvscores = [24.23733196068929, 21.123747159455178, 22.13491161397876, 21.832330076220718, 22.229381039283844]\n",
    "    - average = 22.31154036992556\n",
    "- 更に、`year`を`int64`から`category`に変換した。\n",
    "    - cvscores = [24.36687527029628, 21.106846159060602, 22.053054338444102, 21.826410103971394, 22.33575695373084]\n",
    "    - average = 22.337788565100645\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d46a8-69a9-4236-a380-022c96cf107e",
   "metadata": {},
   "source": [
    "## ideas & todos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f6f9a-1e86-4eac-a568-71532f6ed1dc",
   "metadata": {},
   "source": [
    "- ぞれぞれのcntをどうやって使うか？\n",
    "- monthとdayは通算日を使ったほうがいい？\n",
    "- yearはint64とせず、categoricalとしたほうがいい。-> あんまり変わらん。\n",
    "- cntは消さない方がいいかも。\n",
    "- 年を通算日とした。\n",
    "- pseudo labelingの勉強と、お試し。\n",
    "- pytorch lightningの結果とensamble\n",
    "- cvスコアが22を切り出すと、MLflowで実験管理をする。\n",
    "- Feature importanceの確認\n",
    "- optunaでハイパラのベイズ最適化\n",
    "- tabnetを使ってみる。\n",
    "- tabnetが使えそうであれば、ensamble\n",
    "- ケッペンの気候区分の特徴量を入れてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fce1758-d782-4835-a4f7-60045f100451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.37494306 36.42463268 26.50811286 ... 65.03478816 35.72041752\n",
      " 38.73329297]\n",
      "None\n",
      "[18.37494306 36.42463268 26.50811286 ... 65.03478816 35.72041752\n",
      " 38.73329297]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_lgbm)\n",
    "print(y_pred_tabnet)\n",
    "print(y_pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d31778-b732-475f-844b-ff9b7e9917f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
